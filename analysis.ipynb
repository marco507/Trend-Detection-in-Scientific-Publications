{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing of libraries\nimport pandas as pd\nimport json\nfrom datetime import datetime as dt\nimport matplotlib.pyplot as plt\nfrom matplotlib.dates import DateFormatter\nfrom textblob.en.np_extractors import FastNPExtractor\nimport os\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nimport csv\nimport pickle\nfrom wordcloud import WordCloud\n\n# List all files in the input directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-14T12:10:59.693222Z","iopub.execute_input":"2021-08-14T12:10:59.693774Z","iopub.status.idle":"2021-08-14T12:10:59.708091Z","shell.execute_reply.started":"2021-08-14T12:10:59.693724Z","shell.execute_reply":"2021-08-14T12:10:59.706927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Overview of the data\nimport pandas as pd\n\ndata = pd.read_json('../input/arxiv/arxiv-metadata-oai-snapshot.json', lines=True, nrows=1)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-14T07:46:07.987072Z","iopub.execute_input":"2021-08-14T07:46:07.98745Z","iopub.status.idle":"2021-08-14T07:46:08.071402Z","shell.execute_reply.started":"2021-08-14T07:46:07.987404Z","shell.execute_reply":"2021-08-14T07:46:08.07008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detail view of the publication dates (v1 = Publication Date)\nprint(data['versions'])","metadata":{"execution":{"iopub.status.busy":"2021-08-14T07:46:14.344692Z","iopub.execute_input":"2021-08-14T07:46:14.345045Z","iopub.status.idle":"2021-08-14T07:46:14.3521Z","shell.execute_reply.started":"2021-08-14T07:46:14.345014Z","shell.execute_reply":"2021-08-14T07:46:14.350957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading of the complete dataset filtered by necessary attributes\ndata = {'title': [], 'categories': [], 'abstract': [], 'publication_date': []}\n\nwith open('../input/arxiv/arxiv-metadata-oai-snapshot.json') as f:\n    for line in f:\n        data['title'].append(json.loads(line)['title'])\n        data['categories'].append(json.loads(line)['categories'])\n        data['abstract'].append(json.loads(line)['abstract'])\n        data['publication_date'].append(dt.strptime(\":\".join(json.loads(line)['versions'][0]['created'][5:16].split(\" \")[0:3]), '%d:%b:%Y').date())\n\n# Conversion of the dictionary into a dataframe\ndf = pd.DataFrame(data)\n\n# Check for null values in the entire dataframe\ndf.isnull().values.any()","metadata":{"execution":{"iopub.status.busy":"2021-08-14T12:11:06.204918Z","iopub.execute_input":"2021-08-14T12:11:06.205312Z","iopub.status.idle":"2021-08-14T12:13:33.527412Z","shell.execute_reply.started":"2021-08-14T12:11:06.205279Z","shell.execute_reply":"2021-08-14T12:13:33.526443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the number of entries in the dataframe\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-14T12:13:33.529205Z","iopub.execute_input":"2021-08-14T12:13:33.52961Z","iopub.status.idle":"2021-08-14T12:13:33.535733Z","shell.execute_reply.started":"2021-08-14T12:13:33.529566Z","shell.execute_reply":"2021-08-14T12:13:33.534726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dictionary with the number of publications per category\nnumber_of_pub = {'Computer Science': df[df['categories'].str.contains(pat = 'cs.[A-Z]', regex = True)]['title'].size,\n                 'Economics': df[df['categories'].str.contains(pat = 'econ.')]['title'].size, \n                 'Engineering': df[df['categories'].str.contains(pat = 'eess.')]['title'].size, \n                 'Math': df[df['categories'].str.contains(pat = 'math.[A-Z]')]['title'].size,\n                 'Physics': df[df['categories'].str.contains(pat=('astro-|cond-|gr-qc|hep-|math-ph|nlin|nucl-|physics.|quant-'), regex = True)]['title'].size, \n                 \"Quant-Biology\": df[df['categories'].str.contains(pat = 'q-bio.')]['title'].size, \n                 'Quant-Finance': df[df['categories'].str.contains(pat = 'q-fin.')]['title'].size, \n                 'Statistics': df[df['categories'].str.contains(pat = 'stat.[A-Z]', regex = True)]['title'].size\n                }\n\n# Bar graph to display the number of publications per category\nfields = list(number_of_pub.keys())\nvalues = list(number_of_pub.values())\n\nfig = plt.figure(figsize = (12, 6)) \n \nplt.bar(fields, values, color ='maroon',\n        width = 0.4)\n\nplt.ticklabel_format(axis='y', style='plain') \nplt.xlabel(\"Scientific Fields\")\nplt.ylabel(\"Number of Papers\")\nplt.savefig('pub_per_cat.pdf')  \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-14T12:13:33.537574Z","iopub.execute_input":"2021-08-14T12:13:33.537902Z","iopub.status.idle":"2021-08-14T12:13:40.769502Z","shell.execute_reply.started":"2021-08-14T12:13:33.537873Z","shell.execute_reply":"2021-08-14T12:13:40.768278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create new DataFrames per category, restrict to publications after the year 1999, group by publication date and calculate a moving average of the last 3 months\n# Computer Science\ncs = df[df['categories'].str.contains(pat = 'cs.[A-Z]', regex = True)][['title', 'publication_date']]\ncs = cs[cs['publication_date'] > dt(1999,12,31).date()]\ncs = cs.groupby(\"publication_date\")[\"title\"].count()\ncs = cs.rolling(window=120).mean()\n\n# Physics\nphysics = df[df['categories'].str.contains(pat=('astro-|cond-|gr-qc|hep-|math-ph|nlin|nucl-|physics.|quant-'), regex = True)][['title', 'publication_date']]\nphysics = physics[physics['publication_date'] > dt(1999,12,31).date()]\nphysics = physics.groupby(\"publication_date\")[\"title\"].count()\nphysics = physics.rolling(window=120).mean()\n\n# Mathematics\nmath = df[df['categories'].str.contains(pat = 'math.[A-Z]')][['title', 'publication_date']]\nmath = math[math['publication_date'] > dt(1999,12,31).date()]\nmath = math.groupby(\"publication_date\")[\"title\"].count()\nmath = math.rolling(window=120).mean()\n\n# Economics\necon = df[df['categories'].str.contains(pat = 'econ.')][['title', 'publication_date']]\necon = econ[econ['publication_date'] > dt(1999,12,31).date()]\necon = econ.groupby(\"publication_date\")[\"title\"].count()\necon = econ.rolling(window=120).mean()\n\n# Engineering\neess = df[df['categories'].str.contains(pat = 'eess.')][['title', 'publication_date']]\neess = eess[eess['publication_date'] > dt(1999,12,31).date()]\neess = eess.groupby(\"publication_date\")[\"title\"].count()\neess = eess.rolling(window=120).mean()\n\n# Statistics\nstat = df[df['categories'].str.contains(pat = 'stat.[A-Z]', regex = True)][['title', 'publication_date']]\nstat = stat[stat['publication_date'] > dt(1999,12,31).date()]\nstat = stat.groupby(\"publication_date\")[\"title\"].count()\nstat = stat.rolling(window=120).mean()\n\n# Quantitative Finance\nq_fin = df[df['categories'].str.contains(pat = 'q-fin.')][['title', 'publication_date']]\nq_fin = q_fin[q_fin['publication_date'] > dt(1999,12,31).date()]\nq_fin = q_fin.groupby(\"publication_date\")[\"title\"].count()\nq_fin = q_fin.rolling(window=120).mean()\n\n# Quantitative Biology\nq_bio = df[df['categories'].str.contains(pat = 'q-bio.')][['title', 'publication_date']]\nq_bio = q_bio[q_bio['publication_date'] > dt(1999,12,31).date()]\nq_bio = q_bio.groupby(\"publication_date\")[\"title\"].count()\nq_bio = q_bio.rolling(window=120).mean()\n\n# Display the publications as a time series starting in the year 2001\nfig, axs = plt.subplots(4, 2,figsize=(15,15))\n\n# Format the x-axis\nx_ticks = [dt(year, 1, 1) for year in range(2001, 2022,2)]\ndate_form = DateFormatter(\"%Y\")\n\nfor ax in axs:\n    for plot in ax:\n        plot.xaxis.set_major_formatter(date_form)\n        plot.set_xticks(x_ticks)\n\n# Format the x-axis of Engineering and Economy seperately\nx_ticks = [dt(year, 1, 1) for year in range(2016, 2022,1)]\naxs[2,0].set_xticks(x_ticks)\naxs[2,1].set_xticks(x_ticks)\n\naxs[0, 0].plot(cs, color='maroon')\naxs[0, 0].set_title('Computer Science')\naxs[0, 1].plot(physics, color='maroon')\naxs[0, 1].set_title('Physics')\naxs[1, 0].plot(math, color='maroon')\naxs[1, 0].set_title('Mathematics')\naxs[1, 1].plot(stat, color='maroon')\naxs[1, 1].set_title('Statistics')\naxs[2, 0].plot(econ, color='maroon')\naxs[2, 0].set_title('Economy')\naxs[2, 1].plot(eess, color='maroon')\naxs[2, 1].set_title('Electrical Engineering and Systems Science')\naxs[3, 0].plot(q_bio, color='maroon')\naxs[3, 0].set_title('Quantitative Biology')\naxs[3, 1].plot(q_fin, color='maroon')\naxs[3, 1].set_title('Quantitative Finance')\n\nplt.savefig('pub_trend.pdf')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-14T12:16:49.849943Z","iopub.execute_input":"2021-08-14T12:16:49.850324Z","iopub.status.idle":"2021-08-14T12:16:59.670879Z","shell.execute_reply.started":"2021-08-14T12:16:49.850288Z","shell.execute_reply":"2021-08-14T12:16:59.66985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a Computer Science and Physics DataFrame for Topic Modelling \ncs = df[df['categories'].str.contains(pat = 'cs.[A-Z]', regex = True)][['abstract', 'categories', 'publication_date']]\ncs = cs[cs['publication_date'] > dt(2019,12,31).date()]\n\nphysics = df[df['categories'].str.contains(pat=('astro-|cond-|gr-qc|hep-|math-ph|nlin|nucl-|physics.|quant-'), regex = True)][['abstract', 'categories','publication_date']]\nphysics = physics[physics['publication_date'] > dt(2019,12,31).date()]","metadata":{"execution":{"iopub.status.busy":"2021-08-14T12:17:58.519885Z","iopub.execute_input":"2021-08-14T12:17:58.520306Z","iopub.status.idle":"2021-08-14T12:18:01.076332Z","shell.execute_reply.started":"2021-08-14T12:17:58.520268Z","shell.execute_reply":"2021-08-14T12:18:01.075073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract the nouns from the abstracts, remove entities with special characters and convert them to lowercase\ndef process_data(data):\n    \n    extractor = FastNPExtractor()\n    # Extract the nouns from the abstracts\n    abstract_nouns = [extractor.extract(abstract) for abstract in data]\n\n    # Lowercase and filter special characters for the extracted compound nouns \n    special_characters = ['\\\\', '\\'', '\\\"', '}', '{', 'ยง', '$', '%', '/', '?', '[', ']', '(', ')', '.', '^']\n    normalized_abstract_nouns = []\n    for abstract in abstract_nouns:\n        normalized_abstract_nouns.append([noun.lower() for noun in abstract if not any(character in noun for character in special_characters)])\n\n    return [\" \".join(abstract) for abstract in normalized_abstract_nouns]\n\n#cs['nouns'] = process_data(cs['abstract'])\n#physics['nouns'] = process_data(physics['abstract'])\n\n# Dump the generated features as a csv file\n#cs['nouns'].to_csv('cs_nouns.csv')\n#physics['nouns'].to_csv('physics_nouns.csv') ","metadata":{"execution":{"iopub.status.busy":"2021-08-14T07:49:41.854206Z","iopub.execute_input":"2021-08-14T07:49:41.85454Z","iopub.status.idle":"2021-08-14T08:01:40.444102Z","shell.execute_reply.started":"2021-08-14T07:49:41.854507Z","shell.execute_reply":"2021-08-14T08:01:40.442893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Alternatively load the already generated DataFrame from a csv file\ndef read_nouns(filename):\n    \n    nouns = []\n    with open(filename) as file:\n        csv_reader = csv.reader(file, delimiter=',')\n        line_count = 0\n        for row in csv_reader:\n            if line_count == 0:\n                # Skip the first line\n                line_count += 1\n            else:\n                nouns.append(row[1])\n    return nouns\n            \ncs['nouns'] = read_nouns('../input/dlbdsmlusl01/cs_nouns.csv')\nphysics['nouns'] = read_nouns('../input/dlbdsmlusl01/physics_nouns.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:37:07.112871Z","iopub.execute_input":"2021-08-14T11:37:07.11333Z","iopub.status.idle":"2021-08-14T11:37:09.279025Z","shell.execute_reply.started":"2021-08-14T11:37:07.113288Z","shell.execute_reply":"2021-08-14T11:37:09.278129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Vectorize the data with the TF-IDF method\ndef vectorize_data(data):\n    vectorizer = TfidfVectorizer(lowercase=False, norm=False, stop_words='english')\n    return vectorizer.fit_transform(data), vectorizer.get_feature_names()\n\ncs_vec, cs_feature_names = vectorize_data(cs['nouns'])\nphysics_vec, physics_feature_names = vectorize_data(physics['nouns'])","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:43:09.16124Z","iopub.execute_input":"2021-08-14T11:43:09.161701Z","iopub.status.idle":"2021-08-14T11:43:23.088266Z","shell.execute_reply.started":"2021-08-14T11:43:09.161656Z","shell.execute_reply":"2021-08-14T11:43:23.087265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use LDA for topic modelling\ndef topic_modelling(data):\n    lda = LatentDirichletAllocation(n_components = 20)\n    lda.fit(data)\n    return lda\n\n#cs_lda = topic_modelling(cs_vec)\n#physics_lda = topic_modelling(physics_vec)\n\n# Save the models\n#pickle.dump(cs_lda, open('cs_lda.pk', 'wb'))\n#pickle.dump(physics_lda, open('physics_lda.pk', 'wb'))","metadata":{"execution":{"iopub.status.busy":"2021-08-14T09:15:29.332406Z","iopub.execute_input":"2021-08-14T09:15:29.332921Z","iopub.status.idle":"2021-08-14T09:53:39.566733Z","shell.execute_reply.started":"2021-08-14T09:15:29.33288Z","shell.execute_reply":"2021-08-14T09:53:39.565726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Alternatively load the saved models\ncs_lda = pickle.load(open('../input/dlbdsmlusl01/cs_lda.pk', 'rb'))\nphysics_lda = pickle.load(open('../input/dlbdsmlusl01/physics_lda.pk', 'rb'))","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:44:23.44171Z","iopub.execute_input":"2021-08-14T11:44:23.442082Z","iopub.status.idle":"2021-08-14T11:44:24.198587Z","shell.execute_reply.started":"2021-08-14T11:44:23.442049Z","shell.execute_reply":"2021-08-14T11:44:24.197585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the top words of each topic\ndef plot_top_words(model, feature_names, n_top_words, title, filename):\n    fig, axes = plt.subplots(5, 4, figsize=(15, 20), sharex=True)\n    axes = axes.flatten()\n    for topic_idx, topic in enumerate(model.components_):\n        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n        top_features = [feature_names[i] for i in top_features_ind]\n        weights = topic[top_features_ind]\n\n        ax = axes[topic_idx]\n        ax.barh(top_features, weights, height=0.7, color ='maroon')\n        ax.set_title(f'Topic {topic_idx +1}',\n                     fontdict={'fontsize': 30})\n        ax.invert_yaxis()\n        ax.tick_params(axis='both', which='major', labelsize=20)\n        for i in 'top right left'.split():\n            ax.spines[i].set_visible(False)\n        fig.suptitle(title, fontsize=40)\n\n    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n    plt.savefig(filename)\n    plt.show()\n    \nplot_top_words(cs_lda, cs_feature_names, 5, 'Computer Science Topics', 'cs_topics.pdf')","metadata":{"execution":{"iopub.status.busy":"2021-08-14T12:02:20.021099Z","iopub.execute_input":"2021-08-14T12:02:20.021446Z","iopub.status.idle":"2021-08-14T12:02:22.889386Z","shell.execute_reply.started":"2021-08-14T12:02:20.021408Z","shell.execute_reply":"2021-08-14T12:02:22.888364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_top_words(physics_lda, physics_feature_names, 5, 'Physics Topics', 'physics_topics.pdf')","metadata":{"execution":{"iopub.status.busy":"2021-08-14T12:02:35.946707Z","iopub.execute_input":"2021-08-14T12:02:35.947263Z","iopub.status.idle":"2021-08-14T12:02:39.053061Z","shell.execute_reply.started":"2021-08-14T12:02:35.947211Z","shell.execute_reply":"2021-08-14T12:02:39.051917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a Wordcloud with the top categories\ndef create_wordcloud(data, filename):\n    wordcloud = WordCloud(max_font_size=70, max_words=6, background_color=\"azure\").generate(\" \".join(data))\n    wordcloud.to_file(filename)\n    plt.imshow(wordcloud,interpolation=\"bilinear\")\n    plt.axis('off')\n    plt.show()\n    \ncreate_wordcloud(cs['categories'], 'cs_wordcloud.png')","metadata":{"execution":{"iopub.status.busy":"2021-08-14T12:41:41.768474Z","iopub.execute_input":"2021-08-14T12:41:41.768859Z","iopub.status.idle":"2021-08-14T12:41:43.107719Z","shell.execute_reply.started":"2021-08-14T12:41:41.768828Z","shell.execute_reply":"2021-08-14T12:41:43.106822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_wordcloud(physics['categories'],'physics_wordcloud.png' )","metadata":{"execution":{"iopub.status.busy":"2021-08-14T12:41:44.210139Z","iopub.execute_input":"2021-08-14T12:41:44.210649Z","iopub.status.idle":"2021-08-14T12:41:45.79282Z","shell.execute_reply.started":"2021-08-14T12:41:44.210606Z","shell.execute_reply":"2021-08-14T12:41:45.792032Z"},"trusted":true},"execution_count":null,"outputs":[]}]}